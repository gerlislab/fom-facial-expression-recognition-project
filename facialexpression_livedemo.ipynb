{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, MaxPooling2D, Input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "# Model can be found in sharepoint\n",
    "model = load_model('custom_cnn_v2.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740609884.826698 5424106 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "2025-02-26 23:44:46.704739: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 305ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Mediapipe Face Detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "\n",
    "# Create a folder to save images\n",
    "save_folder = \"takenPictures\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "last_capture_time = time.time()\n",
    "capture_interval = 0.5  # Capture every 0.5 seconds\n",
    "\n",
    "emotion_labels = [\"Angry\", \"Happy\", \"Disgust\", \"Fear\", \"Contempt\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "emotion_results = {label: 0 for label in emotion_labels}\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Convert frame to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect faces\n",
    "    results = face_detection.process(rgb_frame)\n",
    "\n",
    "    if results.detections:\n",
    "        # Get the first detected face\n",
    "        detection = results.detections[0]\n",
    "        bboxC = detection.location_data.relative_bounding_box\n",
    "\n",
    "        h, w, _ = frame.shape\n",
    "        x, y, w_box, h_box = int(bboxC.xmin * w), int(bboxC.ymin * h), int(bboxC.width * w), int(bboxC.height * h)\n",
    "\n",
    "        # Extract the face\n",
    "        face_image = frame[y:y + h_box, x:x + w_box]\n",
    "\n",
    "        # Resize the face to (224,224,3)\n",
    "        face_resized = cv2.resize(face_image, (224, 224))\n",
    "\n",
    "        # Save the image every 0.5 seconds\n",
    "        if time.time() - last_capture_time > capture_interval:\n",
    "            file_path = os.path.join(save_folder, f\"face_image.jpg\")\n",
    "            cv2.imwrite(file_path, face_resized)\n",
    "            last_capture_time = time.time()\n",
    "            img = image.load_img(file_path, target_size=(224, 224)) \n",
    "            img_array = image.img_to_array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            datagen = ImageDataGenerator(rescale=1./255)\n",
    "            img_stand = datagen.standardize(img_array)\n",
    "            \n",
    "            pred = model.predict(img_stand)[0] * 100\n",
    "\n",
    "            emotion_results = {label: value + pred[idx] for idx, label in enumerate(emotion_labels)}\n",
    "\n",
    "    # Draw emotion results on the right side\n",
    "    black_overlay = np.zeros((frame.shape[0], 300, 3), dtype=np.uint8)  # Black rectangle for text\n",
    "    start_y = 50\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1\n",
    "    font_thickness = 2\n",
    "    text_white = (255, 255, 255) \n",
    "    text_green = (0, 255, 0)  \n",
    "\n",
    "    sorted_emotions = sorted(emotion_results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for idx, (emotion, value) in enumerate(sorted_emotions):\n",
    "        text_color = text_green if idx == 0 else text_white\n",
    "        text = f\"{emotion}: {value:.2f}%\"\n",
    "        cv2.putText(black_overlay, text, (20, start_y), font, font_scale, text_color, font_thickness)\n",
    "        start_y += 40\n",
    "\n",
    "    frame = np.hstack((frame, black_overlay))\n",
    "\n",
    "    cv2.imshow(\"Emotion Recognition - Press 'q' to exit\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
