{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1739919056265
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow.lite as tflite\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "import mediapipe as mp\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, precision_recall_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AffectNet Dataset\n",
        "\n",
        "The AffectNet dataset can be found at this link: https://www.kaggle.com/datasets/noamsegal/affectnet-training-data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Github Repository\n",
        "\n",
        "The Github repository of this code and project can be found here: https://github.com/gerlislab/fom-facial-expression-recognition-project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ===========================\n",
        "# Data Preprocessing and Augmentation\n",
        "# ===========================\n",
        "\n",
        "def create_data_generators():\n",
        "    \"\"\"\n",
        "    Creates and configures the data generators for training and testing.\n",
        "    \"\"\"\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        brightness_range=[0.7, 1.3],\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        os.path.join(processed_images_path, \"train\"),\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        os.path.join(processed_images_path, \"test\"),\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "    \n",
        "    return train_generator, test_generator\n",
        "\n",
        "# ===========================\n",
        "# Custom CNN Architectures\n",
        "# ===========================\n",
        "\n",
        "def create_custom_cnn():\n",
        "    \"\"\"\n",
        "    Creates a basic custom CNN model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Conv2D(64, (3,3), activation='relu', input_shape=(img_size[0], img_size[1], 3)),\n",
        "        MaxPooling2D((2,2)),\n",
        "        Conv2D(128, (3,3), activation='relu'),\n",
        "        MaxPooling2D((2,2)),\n",
        "        Conv2D(256, (3,3), activation='relu'),\n",
        "        MaxPooling2D((2,2)),\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(train_generator.num_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_custom_cnn_v2():\n",
        "    \"\"\"\n",
        "    Creates an enhanced custom CNN with batch normalization and deeper architecture.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Conv2D(64, (3,3), activation='relu', input_shape=(img_size[0], img_size[1], 3)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2,2)),\n",
        "\n",
        "        Conv2D(128, (3,3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2,2)),\n",
        "\n",
        "        Conv2D(256, (3,3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2,2)),\n",
        "\n",
        "        Conv2D(512, (3,3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2,2)),\n",
        "\n",
        "        Conv2D(1024, (3,3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2,2)),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(1024, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        Dropout(0.3),\n",
        "        Dense(train_generator.num_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# ===========================\n",
        "# Pretrained Model Architectures\n",
        "# ===========================\n",
        "\n",
        "def create_vgg16_model():\n",
        "    \"\"\"\n",
        "    Creates a VGG16-based model with a trainable feature extractor.\n",
        "    \"\"\"\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "    base_model.trainable = True  # Enable training for all layers\n",
        "\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(train_generator.num_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_resnet50_model():\n",
        "    \"\"\"\n",
        "    Creates a ResNet50-based model with a trainable feature extractor.\n",
        "    \"\"\"\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "    base_model.trainable = True  # Enable training for all layers\n",
        "\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(train_generator.num_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_mobilenetv2_model():\n",
        "    \"\"\"\n",
        "    Creates a MobileNetV2-based model with a trainable feature extractor.\n",
        "    \"\"\"\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "    base_model.trainable = True  # Enable training for all layers\n",
        "\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(train_generator.num_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_resnet50_model_v2():\n",
        "    \"\"\"\n",
        "    Creates a ResNet50-based model with full fine-tuning enabled.\n",
        "    \"\"\"\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "    \n",
        "    # Make all layers trainable\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = True\n",
        "    \n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        Flatten(),\n",
        "        BatchNormalization(),\n",
        "        Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        Dropout(0.5),\n",
        "        Dense(train_generator.num_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Model Evaluation and Performance Analysis\n",
        "# ===========================\n",
        "\n",
        "def measure_inference_time(model, model_name, sample_batch=10):\n",
        "    \"\"\"\n",
        "    Measures the average inference time per image for real-time deployment.\n",
        "    \n",
        "    Parameters:\n",
        "    - model: The trained model to evaluate.\n",
        "    - model_name: A string representing the model name.\n",
        "    - sample_batch: Number of images to use for inference measurement.\n",
        "    \n",
        "    Returns:\n",
        "    - avg_time: The average time per image in seconds.\n",
        "    \"\"\"\n",
        "    test_images, _ = next(test_generator)\n",
        "    test_images = test_images[:sample_batch]  # Test only a subset of images\n",
        "    \n",
        "    start_time = time.time()\n",
        "    model.predict(test_images)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    avg_time = (end_time - start_time) / sample_batch\n",
        "    print(f\"Average inference time per image ({model_name}): {avg_time:.4f} seconds\")\n",
        "    return avg_time\n",
        "\n",
        "def plot_confusion_matrix(model, model_name):\n",
        "    \"\"\"\n",
        "    Computes and visualizes the confusion matrix for model evaluation.\n",
        "    \n",
        "    Parameters:\n",
        "    - model: The trained model to evaluate.\n",
        "    - model_name: A string representing the model name.\n",
        "    \"\"\"\n",
        "    y_true = test_generator.classes\n",
        "    y_pred = model.predict(test_generator)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    class_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Classification Report for {model_name}:\")\n",
        "    print(classification_report(y_true, y_pred_classes, target_names=class_labels))\n",
        "\n",
        "def plot_roc_pr_curves(model, model_name):\n",
        "    \"\"\"\n",
        "    Plots the ROC-AUC and Precision-Recall curves for model evaluation.\n",
        "    \n",
        "    Parameters:\n",
        "    - model: The trained model to evaluate.\n",
        "    - model_name: A string representing the model name.\n",
        "    \"\"\"\n",
        "    y_true = test_generator.classes\n",
        "    y_pred = model.predict(test_generator)\n",
        "    y_pred_prob = y_pred  # Softmax outputs probabilities\n",
        "\n",
        "    n_classes = len(test_generator.class_indices)\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    roc_auc = {}\n",
        "    precision = {}\n",
        "    recall = {}\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # ROC Curve\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_true == i, y_pred_prob[:, i])\n",
        "        roc_auc[i] = roc_auc_score(y_true == i, y_pred_prob[:, i])\n",
        "        plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curve - {model_name}')\n",
        "    plt.legend()\n",
        "\n",
        "    # Precision-Recall Curve\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for i in range(n_classes):\n",
        "        precision[i], recall[i], _ = precision_recall_curve(y_true == i, y_pred_prob[:, i])\n",
        "        plt.plot(recall[i], precision[i], label=f'Class {i}')\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(f'Precision-Recall Curve - {model_name}')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_training_history(history_dict):\n",
        "    \"\"\"\n",
        "    Plots training and validation accuracy/loss over epochs for model evaluation.\n",
        "    \n",
        "    Parameters:\n",
        "    - history_dict: A dictionary containing model training histories.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for name, history in history_dict.items():\n",
        "        plt.plot(history.history['accuracy'], label=f'{name} Train')\n",
        "        plt.plot(history.history['val_accuracy'], label=f'{name} Val')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Training & Validation Accuracy')\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for name, history in history_dict.items():\n",
        "        plt.plot(history.history['loss'], label=f'{name} Train')\n",
        "        plt.plot(history.history['val_loss'], label=f'{name} Val')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training & Validation Loss')\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load CSV and Sort Images into Folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1739919056411
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Load the CSV file containing image labels\n",
        "csv_path = \"labels.csv\"  # Path to the labels file\n",
        "image_base_path = os.getcwd()  # Base path for image directory\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Define target directory for sorted images\n",
        "sorted_images_path = os.path.join(image_base_path, \"sorted_images\")\n",
        "os.makedirs(sorted_images_path, exist_ok=True)\n",
        "\n",
        "# Iterate over the dataset and move images to corresponding label directories\n",
        "for index, row in df.iterrows():\n",
        "    img_path = os.path.join(image_base_path, row[\"pth\"])  # Original image path\n",
        "    label = row[\"label\"]  # Extract the correct label\n",
        "    \n",
        "    # Define target directory for the label\n",
        "    target_dir = os.path.join(sorted_images_path, label)\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    \n",
        "    # Define the new image path in the sorted directory\n",
        "    new_img_path = os.path.join(target_dir, os.path.basename(row[\"pth\"]))\n",
        "    \n",
        "    # Move image if it exists\n",
        "    if os.path.exists(img_path):\n",
        "        shutil.move(img_path, new_img_path)\n",
        "        print(f\"Moved: {img_path} -> {new_img_path}\")\n",
        "    else:\n",
        "        print(f\"Missing: {img_path}\")\n",
        "\n",
        "print(\"Image sorting completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Count and Visualize Sorted Images per Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to the sorted images directory\n",
        "sorted_images_path = \"sorted_images\"\n",
        "\n",
        "# Initialize a dictionary to store image counts for each label\n",
        "label_counts = {}\n",
        "\n",
        "# Iterate through all labels (subdirectories)\n",
        "for label in os.listdir(sorted_images_path):\n",
        "    label_path = os.path.join(sorted_images_path, label)\n",
        "    if os.path.isdir(label_path):  # Ensure it's a directory\n",
        "        label_counts[label] = len(os.listdir(label_path))  # Count images\n",
        "\n",
        "# Print image counts per label\n",
        "for label, count in label_counts.items():\n",
        "    print(f\"{label}: {count} images\")\n",
        "\n",
        "# ===========================\n",
        "# Plot Image Distribution per Emotion\n",
        "# ===========================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a bar chart showing the number of images per emotion\n",
        "plt.bar(label_counts.keys(), label_counts.values())\n",
        "plt.xlabel(\"Emotion\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.title(\"Distribution of Images per Emotion\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Augmentation for Better Generalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1739919059596
        }
      },
      "outputs": [],
      "source": [
        "# Define augmentation parameters\n",
        "augmentation = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Target number of images per class\n",
        "target_count = 3000\n",
        "sorted_images_path = \"sorted_images\"  # Ensure this path is correct\n",
        "\n",
        "# Iterate through all labels (classes)\n",
        "for label in os.listdir(sorted_images_path):\n",
        "    label_path = os.path.join(sorted_images_path, label)\n",
        "    images = os.listdir(label_path)\n",
        "    current_count = len(images)\n",
        "    \n",
        "    if current_count < target_count:\n",
        "        images = np.array(images)  # Optimize for fast sampling\n",
        "        print(f\"Generating {target_count - current_count} additional images for {label}...\")\n",
        "\n",
        "        while len(os.listdir(label_path)) < target_count:\n",
        "            img_name = np.random.choice(images)  # Randomly select an image\n",
        "            img_path = os.path.join(label_path, img_name)\n",
        "\n",
        "            image = cv2.imread(img_path)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image = np.expand_dims(image, axis=0)\n",
        "\n",
        "            # Generate augmented image\n",
        "            aug_iter = augmentation.flow(image, batch_size=1)\n",
        "            aug_image = next(aug_iter)[0].astype(np.uint8)\n",
        "\n",
        "            # Save the new augmented image\n",
        "            new_img_name = f\"aug_{len(os.listdir(label_path))}_{img_name}\"\n",
        "            cv2.imwrite(os.path.join(label_path, new_img_name), cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        print(f\"{label} now has {target_count} images!\")\n",
        "    else:\n",
        "        print(f\"{label} already has sufficient images ({current_count}). No augmentation needed.\")\n",
        "\n",
        "print(\"Data augmentation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Normalize and Split Images into Train/Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths for sorted and processed images\n",
        "sorted_images_path = \"sorted_images\"\n",
        "processed_images_path = \"processed_images\"\n",
        "\n",
        "# Target image size (modify if needed)\n",
        "img_size = (224, 224)\n",
        "\n",
        "# Ensure the processed images directory exists\n",
        "os.makedirs(processed_images_path, exist_ok=True)\n",
        "\n",
        "# Train-test split ratio\n",
        "train_ratio = 0.8\n",
        "\n",
        "# Iterate over all labels (classes) in the sorted dataset\n",
        "for label in os.listdir(sorted_images_path):\n",
        "    label_path = os.path.join(sorted_images_path, label)\n",
        "    images = os.listdir(label_path)\n",
        "\n",
        "    # Split images into training and testing sets\n",
        "    train_images, test_images = train_test_split(images, train_size=train_ratio, random_state=42)\n",
        "    \n",
        "    # Process and save images in respective folders (train/test)\n",
        "    for dataset, dataset_name in zip([train_images, test_images], [\"train\", \"test\"]):\n",
        "        target_dir = os.path.join(processed_images_path, dataset_name, label)\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "        \n",
        "        for img_name in dataset:\n",
        "            img_path = os.path.join(label_path, img_name)\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, img_size)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Save the processed image\n",
        "            new_img_path = os.path.join(target_dir, img_name)\n",
        "            cv2.imwrite(new_img_path, img)\n",
        "\n",
        "print(\"Images have been normalized and split into Train/Test sets!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Processed Images with Data Generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to processed images\n",
        "processed_images_path = \"processed_images\"\n",
        "\n",
        "# Image size and batch size\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "# Create data generators for training and testing\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Training data generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(processed_images_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True  # Shuffle training data for better generalization\n",
        ")\n",
        "\n",
        "# Testing data generator\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(processed_images_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False  # No shuffling for test data to maintain order\n",
        ")\n",
        "\n",
        "print(\"Data successfully loaded in Tensor format!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPU Configuration & Mixed Precision Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for available GPUs and enable memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU memory growth enabled!\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# Enable mixed precision training (FP16 instead of FP32 for better performance)\n",
        "set_global_policy('mixed_float16')\n",
        "\n",
        "# ===========================\n",
        "# Initialize and Train Models\n",
        "# ===========================\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Custom CNN\": create_custom_cnn(),\n",
        "    \"VGG16\": create_vgg16_model(),\n",
        "    \"ResNet50\": create_resnet50_model(),\n",
        "    \"MobileNetV2\": create_mobilenetv2_model()\n",
        "}\n",
        "\n",
        "# Dictionary to store training histories\n",
        "history_dict = {}\n",
        "\n",
        "# Number of epochs\n",
        "epochs = 10\n",
        "\n",
        "# Train all models\n",
        "for name, model in models.items():\n",
        "    print(f\"Starting training for {name} model...\")\n",
        "    history = model.fit(train_generator, validation_data=test_generator, epochs=epochs)\n",
        "    history_dict[name] = history\n",
        "    print(f\"{name} model training completed!\")\n",
        "\n",
        "print(\"All models successfully trained and ready for comparison!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialize optimized Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# Load Data Generators\n",
        "# ===========================\n",
        "\n",
        "train_generator, test_generator = create_data_generators()\n",
        "\n",
        "# ===========================\n",
        "# Callbacks for Early Stopping & Learning Rate Reduction\n",
        "# ===========================\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Early Stopping: Stops training if validation loss does not improve for 4 epochs\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "\n",
        "# Learning Rate Reduction: Reduces LR by factor of 0.5 if validation loss stagnates for 2 epochs\n",
        "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
        "\n",
        "# ===========================\n",
        "# Initialize Models\n",
        "# ===========================\n",
        "\n",
        "models = {\n",
        "    \"Custom CNN v2\": create_custom_cnn_v2(),\n",
        "    \"ResNet50 v2\": create_resnet50_model_v2(),\n",
        "}\n",
        "\n",
        "# ===========================\n",
        "# Train Models with Callbacks\n",
        "# ===========================\n",
        "\n",
        "# Dictionary to store training histories\n",
        "history_dict = {}\n",
        "\n",
        "# Number of epochs (increased due to Early Stopping)\n",
        "epochs = 30  \n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Starting training for {name} model...\")\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        validation_data=test_generator,\n",
        "        epochs=epochs,\n",
        "        callbacks=[early_stopping, lr_reduction]  # Apply early stopping and learning rate reduction\n",
        "    )\n",
        "    history_dict[name] = history\n",
        "    print(f\"{name} model training completed!\")\n",
        "\n",
        "print(\"All models successfully trained and ready for comparison!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyze Training & Validation Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Plot training and validation accuracy/loss for all trained models\n",
        "plot_training_history(history_dict)\n",
        "\n",
        "print(\"Training and validation accuracy analyzed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Display Confusion Matrix for Each Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iterate through all trained models and plot their confusion matrices\n",
        "for name, model in models.items():\n",
        "    plot_confusion_matrix(model, name)\n",
        "\n",
        "print(\"Confusion matrices created and analyzed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Display ROC & Precision-Recall Curves for Each Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iterate through all trained models and plot their ROC-AUC and Precision-Recall curves\n",
        "for name, model in models.items():\n",
        "    plot_roc_pr_curves(model, name)\n",
        "\n",
        "print(\"ROC-AUC & Precision-Recall analysis completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Measure Inference Time for All Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionary to store inference times for each model\n",
        "inference_times = {}\n",
        "\n",
        "# Iterate through all trained models and measure their inference time\n",
        "for name, model in models.items():\n",
        "    inference_times[name] = measure_inference_time(model, name)\n",
        "\n",
        "# ===========================\n",
        "# Plot Model Inference Time Comparison\n",
        "# ===========================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(inference_times.keys(), inference_times.values())\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Average Inference Time (Seconds)\")\n",
        "plt.title(\"Model Runtime Analysis\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Runtime analysis completed, and models optimized for real-time performance!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save Trained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the Custom CNN v2 model\n",
        "models[\"Custom CNN v2\"].save(\"custom_cnn_v2.h5\")\n",
        "\n",
        "# Save the ResNet50 v2 model\n",
        "models[\"ResNet50 v2\"].save(\"resnet50_v2.h5\")\n",
        "\n",
        "print(\"Models successfully saved as .h5 files!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Trained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Load the Custom CNN v2 model\n",
        "custom_cnn_model = load_model(\"custom_cnn_v2.h5\")\n",
        "\n",
        "# Load the ResNet50 v2 model\n",
        "resnet50_model = load_model(\"resnet50_v2.h5\")\n",
        "\n",
        "print(\"Models successfully loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy Custom Model to Real Time Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Mediapipe Face Detection\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
        "\n",
        "# Create a folder to save images\n",
        "save_folder = \"takenPictures\"\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "# Open webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "last_capture_time = time.time()\n",
        "capture_interval = 0.5  # Capture every 0.5 seconds\n",
        "\n",
        "emotion_labels = [\"Angry\", \"Happy\", \"Disgust\", \"Fear\", \"Contempt\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
        "emotion_results = {label: 0 for label in emotion_labels}\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Failed to grab frame\")\n",
        "        break\n",
        "\n",
        "    # Convert frame to RGB\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Detect faces\n",
        "    results = face_detection.process(rgb_frame)\n",
        "\n",
        "    if results.detections:\n",
        "        # Get the first detected face\n",
        "        detection = results.detections[0]\n",
        "        bboxC = detection.location_data.relative_bounding_box\n",
        "\n",
        "        h, w, _ = frame.shape\n",
        "        x, y, w_box, h_box = int(bboxC.xmin * w), int(bboxC.ymin * h), int(bboxC.width * w), int(bboxC.height * h)\n",
        "\n",
        "        # Extract the face\n",
        "        face_image = frame[y:y + h_box, x:x + w_box]\n",
        "\n",
        "        # Resize the face to (224,224,3)\n",
        "        face_resized = cv2.resize(face_image, (224, 224))\n",
        "\n",
        "        # Save the image every 0.5 seconds\n",
        "        if time.time() - last_capture_time > capture_interval:\n",
        "            file_path = os.path.join(save_folder, f\"face_image.jpg\")\n",
        "            cv2.imwrite(file_path, face_resized)\n",
        "            last_capture_time = time.time()\n",
        "            img = image.load_img(file_path, target_size=(224, 224)) \n",
        "            img_array = image.img_to_array(img)\n",
        "            img_array = np.expand_dims(img_array, axis=0)\n",
        "            datagen = ImageDataGenerator(rescale=1./255)\n",
        "            img_stand = datagen.standardize(img_array)\n",
        "            \n",
        "            pred = custom_cnn_model.predict(img_stand)[0] * 100\n",
        "\n",
        "            emotion_results = {label: value + pred[idx] for idx, label in enumerate(emotion_labels)}\n",
        "\n",
        "    # Draw emotion results on the right side\n",
        "    black_overlay = np.zeros((frame.shape[0], 300, 3), dtype=np.uint8)  # Black rectangle for text\n",
        "    start_y = 50\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1\n",
        "    font_thickness = 2\n",
        "    text_white = (255, 255, 255) \n",
        "    text_green = (0, 255, 0)  \n",
        "\n",
        "    sorted_emotions = sorted(emotion_results.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    for idx, (emotion, value) in enumerate(sorted_emotions):\n",
        "        text_color = text_green if idx == 0 else text_white\n",
        "        text = f\"{emotion}: {value:.2f}%\"\n",
        "        cv2.putText(black_overlay, text, (20, start_y), font, font_scale, text_color, font_thickness)\n",
        "        start_y += 40\n",
        "\n",
        "    frame = np.hstack((frame, black_overlay))\n",
        "\n",
        "    cv2.imshow(\"Emotion Recognition - Press 'q' to exit\", frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "cv2.waitKey(1) "
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "env-eric"
    },
    "kernelspec": {
      "display_name": "tf_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "de"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
